# Project Optimization Report: Accelerating MLP Inference

This report details the optimization efforts for a Multi-Layer Perceptron (MLP) project, focusing on the significant speedup achieved through **AVX2 vectorization** and the use of **Look-up Tables (LUT)**.

-----

## Performance Analysis (FashionMNIST)

The initial performance evaluation was conducted on the FashionMNIST dataset. We compared three versions of the MLP: a full-precision floating-point model, a weight-only quantized model, and a fully quantized model accelerated with a Look-up Table.

The results clearly demonstrate the effectiveness of the quantization and LUT-based approach.

\--- Performance and Accuracy Comparison (10000 images, Batch Size: 64) ---
| Metric | All Quantized MLP (LUT) | Wt-Only Quant MLP | Full Prec. Float MLP |
| :--- | :--- | :--- | :--- |
| **Total Time (ms)** | 968.9827 | 1107.6604 | 5666.0382 |
| **Avg. Time / iter(ms)** | 0.0969 | 0.1108 | 0.5666 |
| **Accuracy (%)** | 69.2700 | 77.2700 | 88.1200 |

As shown in the table, the **All Quantized MLP (LUT)** is the fastest implementation, approximately **5.85 times faster** than the **Full Precision Float MLP**. While there is a drop in accuracy, which is a typical trade-off in quantization, the performance gain is substantial.

-----

## Memory Optimization

Quantization also leads to a significant reduction in the memory footprint of the model. By representing weights and activations with fewer bits, we can drastically decrease the model size.

\--- Memory Cost Comparison ---
| Model Type | Memory Cost (KB) |
| :--- | :--- |
| Full Prec. Float MLP| 1213.79 |
| Wt-Only Quant MLP | 304.41 |
| All Quantized (LUT) | 61.91 |

The **All Quantized (LUT)** model is approximately **19.6 times smaller** than the full precision model. This makes it highly suitable for deployment on resource-constrained devices. It's also worth noting that the memory cost of the LUT itself is negligible, especially as the matrix sizes grow.

-----

## Scalability and Speed Evaluation

To test the scalability of our optimizations, we evaluated the models with larger dimensions (Input=3200, Hidden=3200, Output=10).

\--- Speed Evaluation Results ---
Number of iterations: 1000

* **Standard Float MLP**: 17122.8 ms
* **Weights-Only Quantized MLP**: 2278.05 ms
* **LUT-based Full Quantized MLP**: 1517.34 ms

The results from `speed_test.cpp` confirm the trend seen with the smaller model. The **LUT-based Full Quantized MLP** remains the top performer, being about **11.3 times faster** than the standard floating-point implementation and **1.5 times faster** than the weights-only quantized version. This demonstrates that the benefits of the LUT-based approach scale effectively with model size.

-----

## Core Optimization Techniques

The remarkable speedup is primarily due to two key optimization techniques:

### 1\. AVX2 Vectorization ðŸš€

We leveraged **Advanced Vector Extensions 2 (AVX2)** to perform parallel computations. Instead of processing single data points, AVX2 allows us to handle multiple data points in a single instruction. This is particularly effective in the context of neural networks, where operations like dot products are inherently parallelizable.

The `CMakeLists.txt` file is configured to enable AVX2 support during compilation:

```cmake
target_compile_options(myQATModel PRIVATE -mavx2 -O3)
```

The core of the AVX2 implementation can be found in `src/kernels.cpp`. For instance, the `weights_only_linear_forward` function uses AVX2 intrinsics (`_mm256_...`) to accelerate the matrix multiplication process. The `avx2_bit_slice_gemm_kernel` is another key function that leverages AVX2 for the LUT-based approach.

### 2\. Look-up Table (LUT) Acceleration âš¡

The most significant optimization comes from the use of a **Look-up Table (LUT)**. In the fully quantized model, both weights and activations are represented as ternary values (-1, 0, 1). We pre-compute the results of all possible multiplication combinations between these ternary values and store them in a LUT.

The LUT is generated by the `build_bit_slice_lut_5x3` function in `src/utils.cpp`. During inference, instead of performing multiplications, the model can simply look up the result from the table. This transforms the computationally expensive multiplication operations into much faster memory access operations. The `lut_linear_forward` function in `src/kernels.cpp` orchestrates this process.

-----

## Future Work: Larger Look-Up Tables

There is potential for even greater acceleration. If the matrix sizes are known and fixed, we could pre-compute a **larger, more specialized Look-up Table**.

The current implementation uses a LUT for 5x3-bit packed values. A larger LUT could, for example, pre-compute the results for bigger blocks of the matrix. This would further reduce the number of operations at runtime. The trade-off would be a larger memory footprint for the LUT itself, but for specific hardware and fixed model architectures, this could be a viable strategy for pushing the performance boundaries even further.