#include <iostream>
#include <fstream>
#include <vector>
#include "qat_unet_model.h" // Your model loading code
#include "kernels.h"      // Where attention_block is

// Helper to load a flat vector into a Tensor
Tensor load_tensor_from_file(const std::string& path, const std::vector<size_t>& shape) {
    std::ifstream file(path);
    std::vector<float> data;
    float val;
    while (file >> val) {
        data.push_back(val);
    }
    Tensor t(shape);
    t.data = data;
    return t;
}

// Helper to save a tensor for comparison
void save_tensor_to_file(const Tensor& t, const std::string& path) {
    std::ofstream file(path);
    Tensor t_cont = t.contiguous(); // Ensure it's contiguous before writing
    for (const auto& val : t_cont.data) {
        file << val << std::endl;
    }
}

int main() {
    // 1. Load the full model to get the attention block layer weights
    QATUNetModel model;
    model.load_model("qat_unet_model_packed.bin"); // Path to your exported model

    // Find the first attention block (you'll need to know its name/path in your model struct)
    // This is an example path, adjust it to your model structure.
    AttentionBlock& cpp_attention_block = *model.downs[3]->attention; // Example

    // 2. Load the input tensor generated by Python
    // Use the exact shape from the Python script's output
    Tensor input_tensor = load_tensor_from_file("attention_input.txt", {1, 256, 16, 16}); // Adjust shape as needed

    // 3. Run the C++ attention_block function
    Tensor output_tensor_cpp = attention_block(input_tensor, cpp_attention_block);

    // 4. Save the C++ output
    save_tensor_to_file(output_tensor_cpp, "attention_output_cpp.txt");

    std::cout << "C++ attention block output saved to attention_output_cpp.txt" << std::endl;
    std::cout << "Now, compare this file with attention_output_python.txt" << std::endl;

    return 0;
}